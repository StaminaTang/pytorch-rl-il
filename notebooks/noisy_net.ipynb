{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rlil\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "from copy import deepcopy\n",
    "from rlil.environments import GymEnvironment, Action\n",
    "from rlil.presets.continuous import td3, noisy_td3\n",
    "from rlil.initializer import set_device\n",
    "from utils.pendulum_render import PendulumRender\n",
    "set_device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----ON_POLICY_MODE: False-----\n",
      "-----N step: 1-----\n",
      "-----Discount factor: 0.99-----\n"
     ]
    }
   ],
   "source": [
    "# set env\n",
    "env = GymEnvironment(\"Pendulum-v0\")\n",
    "renderer = PendulumRender()\n",
    "renderer.add_pendulum(\"greedy\", color=(.8, .3, .3, 0.5))\n",
    "num_noise = 5\n",
    "for i in range(num_noise):\n",
    "    renderer.add_pendulum(\"noise{}\".format(i), color=(.2, 1.0 - i*0.1, .2, 0.5))\n",
    "\n",
    "# load agent\n",
    "agent_fn = td3()# noisy_td3()\n",
    "agent = agent_fn(env)\n",
    "# agent_dir = \"../runs/noisy_net/Pendulum-v0/noisy-td3_ca7c79d_2020-05-13_12:25:22.382949\"\n",
    "# agent.load(agent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "Type: Box(3)\n",
    "\n",
    "Num | Observation  | Min | Max  \n",
    "----|--------------|-----|----   \n",
    "0   | cos(theta)   | -1.0| 1.0\n",
    "1   | sin(theta)   | -1.0| 1.0\n",
    "2   | theta dot    | -8.0| 8.0\n",
    "\n",
    "\n",
    "## Actions\n",
    "\n",
    "Type: Box(1)\n",
    "\n",
    "Num | Action  | Min | Max  \n",
    "----|--------------|-----|----   \n",
    "0   | Joint effort | -2.0| 2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_theta_thetadots(state):\n",
    "    theta = torch.atan2(state.features[:, 1], state.features[:, 0]).item()\n",
    "    theta_dot = state.features[:, -1].item()\n",
    "    return theta, theta_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# render predicted pendulum\n",
    "env.reset()\n",
    "greedy_env = deepcopy(env)\n",
    "envs = [deepcopy(env) for _ in range(num_noise)]\n",
    "\n",
    "greedy_agent = agent.make_lazy_agent(evaluation=True)\n",
    "greedy_agent.set_replay_buffer(env)\n",
    "agents = [agent.make_lazy_agent(evaluation=False) for _ in range(num_noise)]\n",
    "for a in agents:\n",
    "    a.set_replay_buffer(env)\n",
    "\n",
    "for i in range(100):\n",
    "    greedy_action = greedy_agent.act(greedy_env.state, greedy_env.reward)\n",
    "    actions = [a.act(e.state, e.reward) for a, e in zip(agents, envs)]\n",
    "    \n",
    "    # render \n",
    "    renderer.render(\"greedy\", greedy_env._env.state, greedy_action.features.item())\n",
    "    for i in range(num_noise):\n",
    "        renderer.render(\"noise{}\".format(i), envs[i]._env.state, actions[i].features.item())\n",
    "        \n",
    "    time.sleep(1/60)\n",
    "    \n",
    "    # step oracle\n",
    "    greedy_env.step(greedy_action)\n",
    "    for i in range(num_noise):\n",
    "        envs[i].step(actions[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
